{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploratory Data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing necessary packages\nfrom pandas_datareader import data\nfrom fastai.imports import *\n# from structured import add_datepart\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading into pandas dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Memory optimization\n\n# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16\n\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feat_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/train.csv')\nbuilding_metd_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/building_metadata.csv')\nweather_train_df = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_train.csv')\ntrain_feat_df = reduce_mem_usage(train_feat_df, use_float16=True)\nbuilding_metd_df = reduce_mem_usage(building_metd_df, use_float16=True)\nweather_train_df = reduce_mem_usage(weather_train_df, use_float16=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Examining the dataframes before joins"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_feat_df.head(10))\nprint(\"shape: \", train_feat_df.shape)\nprint(train_feat_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(building_metd_df.head(10))\nprint(\"shape: \", building_metd_df.shape)\nprint(building_metd_df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(weather_train_df.head(10))\nprint(\"shape: \", weather_train_df.shape)\nprint(weather_train_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join train_feat_df {shape: (20216100, 4)} with building_metd_df {(1449, 6)} on foreign key building_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_train_df = pd.merge(train_feat_df, building_metd_df, on='building_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Join building_train_df with weather data with join keys as site id and timestamp"},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking object dtypes for datetime type\nprint(building_train_df['timestamp'].apply(type))\nprint(weather_train_df['timestamp'].apply(type))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since str is the type of timestamp, it should be better if we converted this to a datetime type"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_train_df['timestamp'] = pd.to_datetime(building_train_df['timestamp'])\nweather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weather_building_df : wbt_df\nwbt_df = pd.merge(building_train_df, weather_train_df,how='left', on=['site_id', 'timestamp'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore the merged df"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(building_train_df.head(10))\nprint(\"shape: \", building_train_df.shape)\nprint(building_train_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1.5+ GB memory usage - See how you can decrease the memory usage"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(wbt_df.head(10))\nprint(\"shape: \", wbt_df.shape)\nprint(wbt_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have the following dataframes to perform eda with\n\n1. train_feat_df\n2. building_metd_df\n3. weather_train_df\n4. building_train_df\n5. wbt_df"},{"metadata":{},"cell_type":"markdown","source":"Check for duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"List_Of_Df = [train_feat_df,building_metd_df,weather_train_df,building_train_df,wbt_df]\n\ndef duplicates(df):\n    return df.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# can use iter or for loop. use iter for faster processing\n# d=iter(List_Of_Df)\n# d2 = duplicates(next(d))\n# print(d2)\nfor df in List_Of_Df:\n    print(duplicates(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No duplicates found in any ofdataframes, seems to be a clean dataset\n\nFind the Nan/nulls in our columns and rows "},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_values(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in List_Of_Df:\n    print(missing_values(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fair to say that floor count, year_built, cloud coverage, precip_depth_1_hr are the major columns will null values. Reasonable estimations need to figures out for all \n\nfor floor count: we can estimate the average floor count per site id based on floor counts of buildings in a given site id. or replace it by the average of the floor count throughout the dataset. \n\nIts hard to replace year built with estimations but it could be one hot encoded so all null year builts would fall into the same category\n\nCloud coverage will be the average per site for the specific day and so will the precipitation depth. \n\nNote: We will get a better estimation of how to fill up the nulls by looking at the respective distributions over the period of days, months and years"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fromt the fastai/old packages \n\ndef ifnone(a:Any,b:Any)->Any:\n    \"`a` if `a` is not None, otherwise `b`.\"\n    return b if a is None else a\n\ndef make_date(df:DataFrame, date_field:str):\n    \"Make sure `df[field_name]` is of the right date type.\"\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n        \ndef add_datepart(df:DataFrame, field_name:str, prefix:str=None, drop:bool=True, time:bool=False):\n    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n    make_date(df, field_name)\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub('[Dd]ate$', '', field_name))\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Is_month_end', 'Is_month_start', \n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[prefix + n] = getattr(field.dt, n.lower())\n    df[prefix + 'Elapsed'] = field.astype(np.int64) // 10 ** 9\n    if drop: df.drop(field_name, axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_datepart(wbt_df, 'timestamp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"summarizing Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.describe(include = [np.number])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"summarizing categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.describe(include = ['O'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will comeback to evaluate the describe later as for now it does not seem like a lot of outliers are present. "},{"metadata":{},"cell_type":"markdown","source":"lets visualize the target variable i.e. meter_reading"},{"metadata":{"trusted":true},"cell_type":"code","source":"def target_viz(target):\n    plt.figure(figsize = (14, 6))\n    plt.subplot(1,2,1)\n    sns.boxplot(target)\n    plt.subplot(1,2,2)\n    sns.distplot(target, bins=20)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_viz(wbt_df.meter_reading)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Highl skewed towards towards 0."},{"metadata":{},"cell_type":"markdown","source":"Identyfying potential outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def outliers(df, target):\n    stat = target.describe()\n    print(stat)\n    IQR = stat['75%'] - stat['25%']\n    upper = stat['75%'] + 1.5 * IQR\n    lower = stat['25%'] - 1.5 * IQR\n    print('The upper and lower bounds for suspected outliers are {} and {}.'.format(upper, lower))\n    lower_out = df[target < lower]\n    print('Outliers below lower bound \\n', lower_out)\n    upper_out = df[target > upper]\n    print('Outliers above upper bound \\n', upper_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers(wbt_df, wbt_df.meter_reading)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A high volume is above the upper bound. The upper bound is not the standard upper bound for this problem. The threshold for the upper bound has to be changed. How do we find out the upper bound threshold? \n\nLets check for upper bound counts by type of meters as well as by primary use "},{"metadata":{"trusted":true},"cell_type":"code","source":"2473655/20216100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"about 12% {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}"},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.loc[wbt_df.meter_reading > 642.5100000000001, 'meter'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"steam meter is the highest with reading above upper bound, followed by chilledwater, electricity, hot water"},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.loc[wbt_df.meter_reading > 642.5100000000001, 'primary_use'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wbt_df.loc[wbt_df.meter_reading > 642.5100000000001, 'site_id'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Continue from here tomorrow with more data exploration. but Just for fun lets run the h20 automl to get an idea of what the data is throwing at us"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.barplot(x=wbt_df.meter_reading, y=wbt_df.primary_use)\n# plt.show()\n# plt.bar(x_pos, energy, color='green')\n# plt.xlabel(\"Energy Source\")\n# plt.ylabel(\"Energy Output (GJ)\")\n# plt.title(\"Energy output from various fuel sources\")\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.estimators import H2OXGBoostEstimator\nfrom h2o.estimators.deeplearning import H2OAutoEncoderEstimator\nfrom h2o.automl import H2OAutoML\nh2o.init(nthreads = -1, max_mem_size = 8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = h2o.H2OFrame(wbt_df)\ny = \"meter_reading\"\nsplits = df.split_frame(ratios = [0.8], seed = 1)\ntrain = splits[0]\ntest = splits[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aml = H2OAutoML(max_runtime_secs = 60, seed = 1234)\naml.train(y = y, training_frame = train, leaderboard_frame = test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aml.leaderboard.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}